---
layout: page-sidenav
group: "Chapter. 5"
title: "4. The Hessian Matrix"
---

- 지금까지 우리는 backprop 기법을 이용하여 에러 함수에 대한 \\( {\bf w} \\) 1차 미분값을 계산할 수 있었다.
- 이를 확장하여 2차 미분값인 Hessian 행렬을 계산하는 방법을 살펴보도록 하자.

$$\frac{\partial^2 E}{\partial w_{ji} \partial w_{lk} } \qquad{(5.78)}$$

- 여기서 \\( i, j \in \\\{ 1, ..., W \\\} \\) 이고 \\( W \\) 는 모든 weight 와 bias를 포함한다.
- 이 때 각각의 2차 미분 값을 \\( H_{ij} \\) 로 표기하고 이것으로 만들어지는 행렬을 헤시안(Hessian) 행렬 \\( {\bf H} \\) 라고 정의한다.
- 신경망에서 헤시안 행렬은 중요하게 여겨진다.
    - 일부 비선형 최적화 알고리즘에서 에러 곡면의 2차 미분 값을 사용한다.
    - 이미 학습이 완료된 신경망에 조금 변경된 데이터를 입력으로 주고 빠르게 재학습 시킬 때 사용된다.
    - 'pruning' 일고리즘의 일부로 *least significant weights* 를 식별할 때 헤시안 역행렬이 사용된다.
    - 베이지안 신경망을 위한 라플라스 근사식에서 중요한 역할을 차지한다. (5.7절 참고)
- 헤시안 행렬의 연산량은 매우 높다.
    - 신경망에 \\( W \\) 개의 weight 가 존재한다면 헤시안은 \\( W \times W \\) 행렬이 된다.
    - 따라서 연산량은 입력 샘플당 \\( O(W^2) \\) 이 필요하다.

### 5.4.1. 대각 근사 ( Diagonal approximation)

- 헤시안 행렬을 사용하는 많은 경우에서 헤시안 행렬의 역행렬(inverse)을 이용하는 경우가 많다.
- 이 때 정방 행렬인 헤시안 행렬의 역행렬이 존재하려면 (즉, invertible or nonsingular) \\(det({\bf H})\\) 의 값이 \\(0\\) 이 아니어야 한다.
- 이 때 헤시안 행렬을 대각 근사하면 \\(det({\bf H})\\) 가 \\(0\\) 인 경우가 발생하지 않는다.
    - 즉, 대각만 남기고 다른 값들을 모두 0으로 치환
    - 대각 행렬의 경우 역행렬을 구할 수 있음이 보장된다.

$$\frac{\partial^2 E_n}{\partial w^2_{ji}} = \frac{\partial^2 E_n}{\partial a^2_j}z^2_i \qquad{(5.79)}$$

- 참고로 위의 식은 다음과 같이 전개하여 얻은 식이다.

$$\frac{\partial^2 E_n}{\partial w^2_{ji}} = \frac{\partial^2 E_n}{\partial a^2_{j}} \frac{\partial a^2_{j} }{\partial w^2_{ji}} = \frac{\partial^2 E_n}{\partial a^2_j}z^2_i$$


- 식(5.48)과 식((5.49)를 사용하여 식(5.79)의 오른쪽 항을 체인(chain) 형태로 전개할 수 있다.

$$\frac{\partial^2 E_n}{\partial a^2_j} = h'(a_j)^2\sum_k\sum_{k'} w_{kj}w_{k'j}\frac{\partial^2 E_n}{\partial{a_k} \partial{a_{k'}} } + h''(a_j)\sum_k w_{kj}\frac{\partial E_n}{\partial a_k} \qquad{(5.80)}$$

- 참고로 \\(a_k\\) 와 \\(a_{k'}\\) 는 동일한 레이어의 유닛을 의미한다. \\(k\\) 와 \\(k'\\)가 다른 경우를 모두 무시하면 다음 식을 얻게 된다.
    - 즉, 대각만 남기고 다른 요소들은 모두 삭제한다.
    - *Becker & Le Cun* (1989)

$$\frac{\partial^2 E_n}{\partial a^2_j} = h'(a_j)^2\sum_k w^2_{kj}\frac{\partial^2 E_n}{a^2_k} + h''(a_j)\sum_k w_{kj}\frac{\partial E_n}{\partial a_k} \qquad{(5.81)}$$

- 이 때의 연산 비용은 \\(O(W)\\) 가 된다. (실제 헤시안 행렬은 \\(O(W^2)\\)임을 이미 확인했다.)
- 게다가 연산도 쉬운데 backprop 에서 얻어진 에러 \\( \delta \\) 를 이용하여 \\( \frac {\partial^2 E\_n }{\partial a^2\_j} \\) 만 구하면 헤시한 행렬을 구할 수 있다.
- 하지만 현실적으로 헤시안 행렬 자체는 대각행렬만으로 구성되는 경우는 거의 없다.
    - 따라서 헤시안 행렬의 대각 근사 자체를 잘 사용하지 않는다. (왜 다룬거야!)

### 5.4.2. 외적 근사 (Outer product approximation)

- 회귀(regression) 문제로 돌아가 에러 함수를 잠시 꺼내와보자.

$$E = \frac{1}{2} \sum_{n=1}^N (y_n - t_n)^2 \qquad{(5.82)}$$

- 이에 대한 헤시안 행렬은 다음과 같이 구할 수 있다.

$${\bf H} = \nabla \nabla E = \sum_{n=1}^N \nabla y_n (\nabla y_n)^T + \sum_{n=1}^N(y_n - t_n)\nabla\nabla y_n \qquad{(5.83)}$$

- 만약 충분한 데이터로 학습이 잘 이루어져서 네크워크 망의 출력값 \\(y_n\\) 가 \\(t_n\\) 와 매우 비슷한 값을 내어준다면 위의 식에서 2번째 텀은 생략 가능하다.
    - 물론 제대로 하려면 섹션 1.5.5 에서 다루었듯 출력 값의 조건부 평균을 사용해야 겠지만 일단 넘어가자.
    - 식(5.83)에서 두번째 텀을 생략한 식을 *Levenberg-Marquardt* 근사 또는 외적 근사(outer product approx.)라고 부른다.
    - 사실 헤시안 행렬 자체가 외적의 합으로 이루어진 행렬이다.

$${\bf H} \simeq \sum_{n=1}^N {\bf b_n}{\bf b_n}^T \qquad{(5.84)}$$

- 여기서 \\({\bf b_n} \equiv \nabla a\_n = y\_n\\) 이다.
    - 회귀 문제에서는 활성(activation) 함수가 identity 이기 때문이다.

- 이 근사법은 헤시안을 구하기 위해 오로지 1차 미분만을 요구하고 backprop 시 \\(O(W)\\) 에 값을 구할 수 있다.
- 그리고 \\(W\\) 차원의 두 벡터를 외적할 때 \\( O(W^2) \\) 이 요구된다.

- 로지스틱 시그모이드(logistic sigmoid) 활성 함수를 가지는 크로스 엔트로피(cross-entropy)를  사용하는 경우 근사식은 다음과 같이 된다.

$${\bf H} \simeq \sum_{n=1}^N y_n(1-y_n){\bf b_n}{\bf b_n}^T \qquad{(5.85)}$$

- 다중부류(multi-class)로도 쉽게 확장된다.

### 5.4.3. 역 헤시안 (Inverse Hessian)

- 헤시안을 응용하는 곳에서는 주로 역 헤시안을 사용하는 경우가 많다. (신경망에서도!)
- 앞서 살펴보았던 외적 근사 기법을 통해 헤시안의 역행렬을 구하는 방법을 살펴볼 것이다.
- 끝에 나오는 quasi-Newton 을 주의깊게 살펴보자.

$${\bf H}_N = \sum_{n=1}^N {\bf b_n}{\bf b_n}^T \qquad{(5.86)}$$








